% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/performance_metrics.R
\name{evaluate_performance}
\alias{evaluate_performance}
\title{Evaluate predictive performance of all methods and oracle references}
\usage{
evaluate_performance(simulation_object)
}
\arguments{
\item{simulation_object}{List. Output of a single simulation run, containing
the test data, predictions from all methods, oracle reference quantities,
and metadata.}
}
\value{
A nested list with one element per analysis group
(\code{"overall"}, \code{"complete"}, \code{"incomplete"}). Each group
contains named numeric vectors of performance values for the following
metrics:
\describe{
  \item{mse}{Mean squared error with respect to the true outcome \code{Y}.}
  \item{msep_omu}{Mean squared error of prediction with respect to the oracle
    predictor ignoring missingness (\code{refOMU}).}
  \item{msep_omc}{Mean squared error of prediction with respect to the oracle
    predictor conditioning on missingness (\code{refOMC}).}
}
}
\description{
Computes predictive performance metrics for a set of prediction methods and
oracle reference predictors, stratified by missingness pattern.
}
\details{
Performance is evaluated separately for:
\itemize{
  \item all test observations (\code{"overall"}),
  \item observations with fully observed \code{X1} (\code{"complete"}),
  \item observations with missing \code{X1} (\code{"incomplete"}).
}

All estimators and oracle reference predictors (\code{refMU}, \code{refMC})
are evaluated on an equal footing. The function returns no aggregated or
smoothed quantities; it operates at the level of a single simulation run.
}
\seealso{
\code{\link{compute_msd}}, \code{\link{compute_reference_probabilities}}
}
